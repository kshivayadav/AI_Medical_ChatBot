# ğŸ¥ AI Medical Chatbot (RAG-Based)

An **AI-powered Medical Chatbot** built using **FastAPI, LangChain, Pinecone, and Streamlit**, designed to provide **accurate, context-aware medical information** by leveraging **Retrieval-Augmented Generation (RAG)**.

---

## ğŸš€ Project Overview

The AI Medical Chatbot allows users to ask medical-related questions and receive **reliable answers grounded in medical documents**.  
Instead of generating answers purely from a language model, this system **retrieves relevant medical knowledge** and then generates responses based on that context.

---

## ğŸ§  Key Features

- âœ… Retrieval-Augmented Generation (RAG)
- âœ… Context-aware medical responses
- âœ… FastAPI backend (async & scalable)
- âœ… Streamlit-based interactive frontend
- âœ… Pinecone vector database for semantic search
- âœ… Industry-level logging & middleware
- âœ… Clean UI with chat bubbles (User vs Bot)
- âœ… Error handling and validation
- âš ï¸ Educational use only (not medical advice)

---

## ğŸ—ï¸ Tech Stack

### Backend
- **FastAPI** â€“ High-performance REST API
- **LangChain** â€“ RAG pipeline orchestration
- **Pinecone** â€“ Vector database for embeddings
- **HuggingFace / Ollama** â€“ LLM inference
- **Sentence Transformers** â€“ Text embeddings
- **Pydantic** â€“ Data validation

### Frontend
- **Streamlit** â€“ Interactive UI

### Other Tools
- Python 3.10+
- Uvicorn
- dotenv
- PDF loaders

---

## ğŸ“ Project Structure
```bash
AI_Medical_ChatBot/
â”‚
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ src/
â”‚ â”‚ â”œâ”€â”€ main.py # FastAPI entry point
â”‚ â”‚ â”œâ”€â”€ rag_pipeline.py # RAG chain logic
â”‚ â”‚ â”œâ”€â”€ helper.py # PDF loading & embeddings
â”‚ â”‚ â”œâ”€â”€ store_index.py # Pinecone index creation
â”‚ â”‚ â”œâ”€â”€ schema.py # Request & response models
â”‚ â”‚ â”œâ”€â”€ prompt.py # prompt for instructions
â”‚ â”‚ â”œâ”€â”€ config.py # configuring the variables
â”‚ â”‚ â”œâ”€â”€ __init__.py
â”‚ â”‚ â”œâ”€â”€ requirements.txt 
â”œâ”€â”€ Data/
â”‚ â””â”€â”€ Medical_book.pdf # Medical knowledge source
â”‚
â”œâ”€â”€ frontend/
â”‚ â””â”€â”€ app.py # Streamlit UI
â”‚
â”œâ”€â”€ template.sh
â”œâ”€â”€ setup.py
â”œâ”€â”€ .env
â””â”€â”€ README.md
```
## ğŸ›ï¸ Architecture Diagram
```markdown
## ğŸ›ï¸ System Architecture
```
```text
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   User (Browser)   â”‚
                â”‚  Streamlit Frontendâ”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â”‚ HTTP POST (Question)
                          â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   FastAPI Backend    â”‚
                â”‚  (Async + Middleware)â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  RAG Pipeline (LangChain) â”‚
            â”‚  â€¢ Retriever              â”‚
            â”‚  â€¢ Prompt                 â”‚
            â”‚  â€¢ LLM                    â”‚
            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”˜
                   â”‚                  â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Pinecone Vectorâ”‚   â”‚  LLM Engine   â”‚
          â”‚   Database     â”‚   â”‚ (Ollama / HF) â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```



---

## ğŸ”„ How the System Works (RAG Flow)

1. ğŸ“¥ User enters a medical question
2. ğŸ” Relevant documents retrieved from Pinecone
3. ğŸ§  Retrieved context passed to LLM
4. âœï¸ Answer generated using context
5. ğŸ“¤ Clean response returned to UI

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/your-username/AI_Medical_ChatBot.git
cd AI_Medical_ChatBot
```

### 2ï¸âƒ£ Create Virtual Environment
```bash
conda create -n medibot python=3.10
conda activate medibot
```
### 3ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```
## ğŸ” Environment Variables (.env)
```env
PINECONE_API_KEY=your_pinecone_key
PINECONE_ENV=your_environment
PINECONE_INDEX_NAME=medical-chatbot
BACKEND_URL=http://127.0.0.1:8000/medicalchatbot
```
## ğŸ“¦ Create Pinecone Index
```
cd backend/src
python store_index.py
```
This :

- Loads PDFs

- Splits documents

- Generates embeddings

- Stores vectors in Pinecone

### â–¶ï¸ Run the Backend
```bash
cd backend/src
uvicorn main:app --reload
```
API will be available at:
```cpp
http://127.0.0.1:8000
```
### ğŸ–¥ï¸ Run the Frontend
```bash
cd frontend
streamlit run app.py
```
## ğŸ”Œ API Endpoints
| Method | Endpoint          | Description          |
| ------ | ----------------- | -------------------- |
| GET    | `/`               | Root message         |
| GET    | `/health`         | Health check         |
| POST   | `/medicalchatbot` | Ask medical question |

## ğŸ§ª Sample Request
```json
{
  "question": "What is acne?"
}
```
Sample Response
```json
{
  "answer": "Acne is a common skin condition caused by clogged pores..."
}
```

## ğŸ›¡ï¸ Middleware Used

- Request logging

- Response time tracking

- Request ID generation

- Centralized error handling

## âš ï¸ Disclaimer

This chatbot is for educational purposes only.

It is not a substitute for professional medical advice, diagnosis, or treatment.

Always consult a qualified healthcare provider.

## ğŸŒŸ Future Enhancements

- ğŸ” Authentication & rate limiting

- ğŸ“Š Admin analytics dashboard

- ğŸ§¬ More medical datasets

- ğŸŒ Multi-language support

- ğŸ§ª Unit & integration tests

## ğŸ‘¨â€ğŸ’» Author

Shiva Kumar

ğŸ“§ Email: kshivayadav7@gmail.com

ğŸ”— LinkedIn: (https://www.linkedin.com/in/shiva-kumar-5586432b0/)